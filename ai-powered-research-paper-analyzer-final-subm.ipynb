{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d72d5f2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:01.832762Z",
     "iopub.status.busy": "2025-04-20T07:49:01.832354Z",
     "iopub.status.idle": "2025-04-20T07:49:03.919869Z",
     "shell.execute_reply": "2025-04-20T07:49:03.918700Z"
    },
    "papermill": {
     "duration": 2.097213,
     "end_time": "2025-04-20T07:49:03.921541",
     "exception": false,
     "start_time": "2025-04-20T07:49:01.824328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/research-papers-corpus/AdapterFusion_Non-Destructive_Task_Composition_for_Transfer_Learning.pdf\n",
      "/kaggle/input/research-papers-corpus/Parameter-Efficient_Transfer_Learning_with_Adapters.pdf\n",
      "/kaggle/input/research-papers-corpus/Attention_Is_All_You_Need.pdf\n",
      "/kaggle/input/research-papers-corpus/Transformers_for_Low-Resource_Languages_Is_Feidir_Linn.pdf\n",
      "/kaggle/input/research-papers-corpus/Unsupervised_Cross-lingual_Representation_Learning_at_Scale.pdf\n",
      "/kaggle/input/research-papers-corpus/XTREME-R_Towards_More_Challenging_and_Nuanced_Multilingual_Evaluation.pdf\n",
      "/kaggle/input/research-papers-corpus/chrF_Character_n-gram_F-score_for_Automatic_MT_Evaluation.pdf\n",
      "/kaggle/input/research-papers-corpus/Multilingual_Denoising_Pre-training_for_Neural_Machine_Translation.pdf\n",
      "/kaggle/input/research-papers-corpus/BLEU_A_Method_for_Automatic_Evaluation_of_Machine_Translation.pdf\n",
      "/kaggle/input/research-papers-corpus/Simple_Scalable_Adaptation_for_Neural_Machine_Translation.pdf\n",
      "/kaggle/input/sample-paper/Deja_Vu_Multilingual_LLM_Evaluation_through_the_Lens_of_Machine_Translation_Evaluation.pdf\n",
      "/kaggle/input/metadata/corpus_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b544c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:03.932316Z",
     "iopub.status.busy": "2025-04-20T07:49:03.931851Z",
     "iopub.status.idle": "2025-04-20T07:49:37.527633Z",
     "shell.execute_reply": "2025-04-20T07:49:37.525943Z"
    },
    "papermill": {
     "duration": 33.603034,
     "end_time": "2025-04-20T07:49:37.529475",
     "exception": false,
     "start_time": "2025-04-20T07:49:03.926441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\r\n",
      "Collecting chromadb\r\n",
      "  Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\r\n",
      "Collecting pypdf2\r\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Collecting rouge-score\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\r\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\r\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\r\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\r\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\r\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\r\n",
      "Collecting build>=1.0.3 (from chromadb)\r\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\r\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\r\n",
      "Collecting fastapi==0.115.9 (from chromadb)\r\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\r\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\r\n",
      "Collecting posthog>=2.4.0 (from chromadb)\r\n",
      "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\r\n",
      "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\r\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\r\n",
      "Collecting pypika>=0.48.9 (from chromadb)\r\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\r\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\r\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\r\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\r\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\r\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\r\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\r\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\r\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\r\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\r\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\r\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\r\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\r\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\r\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\r\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\r\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.67.0)\r\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\r\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\r\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\r\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\r\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\r\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\r\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\r\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.1.0)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-proto==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.32.0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-proto==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.31.0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting opentelemetry-proto==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "INFO: pip is still looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting opentelemetry-proto==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\r\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\r\n",
      "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.53b0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.53b0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.53b0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-util-http==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.53b0-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.32.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.31.1-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.52b0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-util-http==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.52b0-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.31.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\r\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b1-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b1-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-util-http==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.28.1-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "INFO: pip is still looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b0-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-util-http==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.49b0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.28.0-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\r\n",
      "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\r\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\r\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\r\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\r\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\r\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\r\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\r\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\r\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\r\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\r\n",
      "Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\r\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\r\n",
      "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\r\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\r\n",
      "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\r\n",
      "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\r\n",
      "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\r\n",
      "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\r\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\r\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\r\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\r\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\r\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\r\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge-score, pypika\r\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2b1e7a44b913b7f8c8571772c430227d9b14c358bbe3ab1f37a5396788733bda\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\r\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=90ed1c121ceec6bbe01e34e32439263b6e1dfb8ac4c5323afdc82de6d0eb8f64\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\r\n",
      "Successfully built rouge-score pypika\r\n",
      "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, pypdf2, opentelemetry-util-http, opentelemetry-proto, mmh3, importlib-metadata, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-instrumentation, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, onnxruntime, chroma-hnswlib, rouge-score, chromadb\r\n",
      "  Attempting uninstall: importlib-metadata\r\n",
      "    Found existing installation: importlib_metadata 8.6.1\r\n",
      "    Uninstalling importlib_metadata-8.6.1:\r\n",
      "      Successfully uninstalled importlib_metadata-8.6.1\r\n",
      "  Attempting uninstall: opentelemetry-api\r\n",
      "    Found existing installation: opentelemetry-api 1.16.0\r\n",
      "    Uninstalling opentelemetry-api-1.16.0:\r\n",
      "      Successfully uninstalled opentelemetry-api-1.16.0\r\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\r\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\r\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\r\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\r\n",
      "  Attempting uninstall: opentelemetry-sdk\r\n",
      "    Found existing installation: opentelemetry-sdk 1.16.0\r\n",
      "    Uninstalling opentelemetry-sdk-1.16.0:\r\n",
      "      Successfully uninstalled opentelemetry-sdk-1.16.0\r\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.5 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.4.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.1 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 posthog-3.25.0 pypdf2-3.0.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 rouge-score-0.1.2 starlette-0.45.3 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\r\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install google-generativeai chromadb pypdf2 pandas rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a58049e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:37.560788Z",
     "iopub.status.busy": "2025-04-20T07:49:37.560259Z",
     "iopub.status.idle": "2025-04-20T07:49:44.164637Z",
     "shell.execute_reply": "2025-04-20T07:49:44.163572Z"
    },
    "papermill": {
     "duration": 6.621729,
     "end_time": "2025-04-20T07:49:44.166360",
     "exception": false,
     "start_time": "2025-04-20T07:49:37.544631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libaries\n",
    "import google.generativeai as genai\n",
    "from pprint import pprint\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from chromadb import Client\n",
    "import textwrap\n",
    "from rouge_score import rouge_scorer\n",
    "from PyPDF2 import PdfReader\n",
    "from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53859a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.196855Z",
     "iopub.status.busy": "2025-04-20T07:49:44.196302Z",
     "iopub.status.idle": "2025-04-20T07:49:44.264826Z",
     "shell.execute_reply": "2025-04-20T07:49:44.263822Z"
    },
    "papermill": {
     "duration": 0.085807,
     "end_time": "2025-04-20T07:49:44.266799",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.180992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Gemini API key from Kaggle Secrets\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error loading API key: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428a3d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.297917Z",
     "iopub.status.busy": "2025-04-20T07:49:44.297572Z",
     "iopub.status.idle": "2025-04-20T07:49:44.302153Z",
     "shell.execute_reply": "2025-04-20T07:49:44.301395Z"
    },
    "papermill": {
     "duration": 0.022219,
     "end_time": "2025-04-20T07:49:44.303650",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.281431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Gemini model\n",
    "try:\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20df010f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.335720Z",
     "iopub.status.busy": "2025-04-20T07:49:44.335383Z",
     "iopub.status.idle": "2025-04-20T07:49:44.343795Z",
     "shell.execute_reply": "2025-04-20T07:49:44.342990Z"
    },
    "papermill": {
     "duration": 0.026489,
     "end_time": "2025-04-20T07:49:44.345226",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.318737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text content from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text content from the PDF.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the PDF file is not found at the specified path.\n",
    "        Exception: If an error occurs during the PDF reading process.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PdfReader(pdf_file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() or \"\"\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading PDF {pdf_path}: {e}\")\n",
    "\n",
    "def create_document(pdf_path: str, title: str, abstract: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a document dictionary containing the full text and metadata of a PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        title (str): The title of the document.\n",
    "        abstract (str): The abstract of the document.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'content' (full text from PDF) and 'metadata'\n",
    "              (a dictionary containing 'title', 'abstract', and 'filename').\n",
    "    \"\"\"\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    metadata = {\"title\": title, \"abstract\": abstract, \"filename\": os.path.basename(pdf_path)}\n",
    "    return {\"content\": full_text, \"metadata\": metadata}\n",
    "\n",
    "def extract_sample_pdf_text(pdf_path: str, max_chars: int = 6000) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a sample of text from the beginning of a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        max_chars (int, optional): The maximum number of characters to extract. Defaults to 6000.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the first 'max_chars' characters of the PDF text,\n",
    "             or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_text = extract_text_from_pdf(pdf_path)\n",
    "        return full_text[:max_chars]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting sample text from {pdf_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac6f5eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.375692Z",
     "iopub.status.busy": "2025-04-20T07:49:44.375355Z",
     "iopub.status.idle": "2025-04-20T07:49:44.381302Z",
     "shell.execute_reply": "2025-04-20T07:49:44.380470Z"
    },
    "papermill": {
     "duration": 0.023113,
     "end_time": "2025-04-20T07:49:44.382844",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.359731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Structured Output ---\n",
    "def generate_structured_output(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a structured summary of a research paper from its text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text content of the research paper.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the structured information extracted from the text,\n",
    "              including 'title', 'authors', 'key_findings', 'methodology', and 'implications'.\n",
    "              Returns an empty dictionary if JSON decoding fails.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at summarizing research papers. Extract the following information from the text provided:\n",
    "    \n",
    "    {text[:6000]}\n",
    "    \n",
    "    Format the output as a JSON object with the following keys:\n",
    "    'title': The title of the paper.\n",
    "    'authors': A list of the paper's authors (if available, else 'Not specified').\n",
    "    'key_findings': A bulleted list of the most important findings.\n",
    "    'methodology': A concise description of the main methods used.\n",
    "    'implications': A brief summary of the significance and potential impact of the research.\n",
    "    \"\"\"\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "    try:\n",
    "        return json.loads(response.text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON: {response.text}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065c341e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.413640Z",
     "iopub.status.busy": "2025-04-20T07:49:44.413231Z",
     "iopub.status.idle": "2025-04-20T07:49:44.419109Z",
     "shell.execute_reply": "2025-04-20T07:49:44.418309Z"
    },
    "papermill": {
     "duration": 0.023226,
     "end_time": "2025-04-20T07:49:44.420670",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.397444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 2. Few-Shot Prompting ---\n",
    "def few_shot_summary(sample_abstracts: list, current_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a concise 1-sentence summary of a research paper using few-shot prompting.\n",
    "\n",
    "    Args:\n",
    "        sample_abstracts (list): A list of dictionaries, where each dictionary contains\n",
    "                                  at least an 'abstract' key with a sample abstract.\n",
    "        current_text (str): The text content of the research paper to summarize.\n",
    "\n",
    "    Returns:\n",
    "        str: A 1-sentence summary of the provided research paper content.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a research assistant skilled at summarizing papers. Below are example summaries of research papers related to multilingual language models and evaluation. Each summary is concise (1 sentence) and focuses on the main contribution:\n",
    "    \n",
    "    Example 1:\n",
    "    Abstract: {sample_abstracts[0]['abstract'][:180]}...\n",
    "    Summary: This paper proposes ethical guidelines for AI systems to ensure fairness and transparency.\n",
    "    \n",
    "    Example 2:\n",
    "    Abstract: {sample_abstracts[1]['abstract'][:180]}...\n",
    "    Summary: This paper advocates adopting machine translation evaluation methods to improve multilingual LLM evaluation.\n",
    "    \n",
    "    Now, provide a concise 1-sentence summary of the following research paper, focusing on its main contribution:\n",
    "    \n",
    "    Content: {current_text[:2000]}...\n",
    "    \n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb721ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.451209Z",
     "iopub.status.busy": "2025-04-20T07:49:44.450848Z",
     "iopub.status.idle": "2025-04-20T07:49:44.456333Z",
     "shell.execute_reply": "2025-04-20T07:49:44.455446Z"
    },
    "papermill": {
     "duration": 0.022568,
     "end_time": "2025-04-20T07:49:44.457815",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.435247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. Document Understanding ---\n",
    "def document_understanding(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes research paper text to extract key findings, methodology, and a simplified explanation.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text content of the research paper.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing a structured breakdown of the paper, including\n",
    "             'Key Findings', 'Methodology', and 'In Simpler Terms'.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following research paper text and provide a structured breakdown:\n",
    "    \n",
    "    {text[:6000]}\n",
    "    \n",
    "    Include:\n",
    "    **Key Findings:** A bulleted list of the most important findings.\n",
    "    **Methodology:** A brief paragraph explaining the primary research methods used.\n",
    "    **In Simpler Terms:** A concise paragraph explaining the paper's main argument to a non-expert.\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a5f526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.488797Z",
     "iopub.status.busy": "2025-04-20T07:49:44.488465Z",
     "iopub.status.idle": "2025-04-20T07:49:44.494117Z",
     "shell.execute_reply": "2025-04-20T07:49:44.493179Z"
    },
    "papermill": {
     "duration": 0.022727,
     "end_time": "2025-04-20T07:49:44.495699",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.472972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 4. Agents (Simulated with Direct Function Call) ---\n",
    "def agent_retrieve_paper(query: str, chroma_client: Client, k: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Simulates an agent retrieving a relevant research paper from a vector store based on a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find relevant papers.\n",
    "        chroma_client (Client): The ChromaDB client connected to the vector store.\n",
    "        k (int, optional): The number of top results to retrieve. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing information about the retrieved paper (content preview and metadata)\n",
    "             if found, \"Vector store not initialized.\" if the client is None, or\n",
    "             \"No relevant papers found.\" if no results are returned.\n",
    "    \"\"\"\n",
    "    if not chroma_client:\n",
    "        return \"Vector store not initialized.\"\n",
    "    results = vector_search_improved(chroma_client, query, k=k)\n",
    "    if results:\n",
    "        top_result = results[0]\n",
    "        return f\"\"\"\n",
    "        Retrieved Paper:\n",
    "        Content Preview: {textwrap.shorten(top_result['document'], width=600, placeholder='...')}\n",
    "        Metadata: {json.dumps(top_result['metadata'], indent=2)}\n",
    "        \"\"\"\n",
    "    return \"No relevant papers found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c32b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.526423Z",
     "iopub.status.busy": "2025-04-20T07:49:44.526069Z",
     "iopub.status.idle": "2025-04-20T07:49:44.531355Z",
     "shell.execute_reply": "2025-04-20T07:49:44.530263Z"
    },
    "papermill": {
     "duration": 0.022363,
     "end_time": "2025-04-20T07:49:44.532946",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.510583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 5. Long Context Window ---\n",
    "def long_context_summary(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a detailed summary of a research paper, designed for longer text inputs.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full text content of the research paper.\n",
    "\n",
    "    Returns:\n",
    "        str: A detailed summary of the key findings and implications of the paper.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the key findings and implications of the following research paper in detail:\n",
    "    \n",
    "    {text[:10000]}\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83aa5bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.563235Z",
     "iopub.status.busy": "2025-04-20T07:49:44.562858Z",
     "iopub.status.idle": "2025-04-20T07:49:44.568204Z",
     "shell.execute_reply": "2025-04-20T07:49:44.567323Z"
    },
    "papermill": {
     "duration": 0.022235,
     "end_time": "2025-04-20T07:49:44.569653",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.547418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 6. Context Caching (Simulated) ---\n",
    "def context_caching_summary(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulates context caching by generating a concise summary of the research paper.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text content of the research paper.\n",
    "\n",
    "    Returns:\n",
    "        str: A concise summary of the core argument and key contributions of the paper.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Provide a concise summary of the core argument and key contributions of the following research paper:\n",
    "    \n",
    "    {text[:6000]}\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb6e4e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.600827Z",
     "iopub.status.busy": "2025-04-20T07:49:44.600047Z",
     "iopub.status.idle": "2025-04-20T07:49:44.605526Z",
     "shell.execute_reply": "2025-04-20T07:49:44.604735Z"
    },
    "papermill": {
     "duration": 0.02281,
     "end_time": "2025-04-20T07:49:44.606842",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.584032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 7. Gen AI Evaluation ---\n",
    "def evaluate_summary(ground_truth: str, model_summary: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a generated summary against a ground truth summary using ROUGE scores.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (str): The reference (ground truth) summary.\n",
    "        model_summary (str): The summary generated by the language model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing ROUGE scores (rouge1 and rougeL) as dictionaries\n",
    "              with 'precision', 'recall', and 'fmeasure'.\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth, model_summary)\n",
    "    return {metric: score._asdict() for metric, score in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b5f6cae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.637632Z",
     "iopub.status.busy": "2025-04-20T07:49:44.637288Z",
     "iopub.status.idle": "2025-04-20T07:49:44.642392Z",
     "shell.execute_reply": "2025-04-20T07:49:44.641536Z"
    },
    "papermill": {
     "duration": 0.022136,
     "end_time": "2025-04-20T07:49:44.643729",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.621593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 8. Grounding ---\n",
    "def grounding_analysis(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes research paper text to identify and explain references to prior work.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text content of the research paper.\n",
    "\n",
    "    Returns:\n",
    "        str: A string explaining how the paper builds upon, compares to, or differs\n",
    "             from existing research, benchmarks, or datasets mentioned in the text.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following research paper text. Identify mentions of existing research, benchmarks, or datasets. Explain how the paper builds upon, compares to, or differs from these prior works:\n",
    "    \n",
    "    {text[:6000]}\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48a74743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.674316Z",
     "iopub.status.busy": "2025-04-20T07:49:44.673968Z",
     "iopub.status.idle": "2025-04-20T07:49:44.679393Z",
     "shell.execute_reply": "2025-04-20T07:49:44.678484Z"
    },
    "papermill": {
     "duration": 0.022738,
     "end_time": "2025-04-20T07:49:44.680825",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.658087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 9. Embeddings ---\n",
    "def generate_embeddings(text_list: list[str]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of text strings using the specified embedding model.\n",
    "\n",
    "    Args:\n",
    "        text_list (list[str]): A list of text strings to embed.\n",
    "\n",
    "    Returns:\n",
    "        list[list[float]]: A list of embeddings, where each embedding is a list of floats\n",
    "                           corresponding to the input text.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text in text_list:\n",
    "        response = genai.embed_content(\n",
    "            model=\"models/embedding-001\",\n",
    "            content=text[:8192],  # Limit to max input length\n",
    "            task_type=\"retrieval_document\"\n",
    "        )\n",
    "        embeddings.append(response[\"embedding\"])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6a097c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.711000Z",
     "iopub.status.busy": "2025-04-20T07:49:44.710677Z",
     "iopub.status.idle": "2025-04-20T07:49:44.717407Z",
     "shell.execute_reply": "2025-04-20T07:49:44.716204Z"
    },
    "papermill": {
     "duration": 0.023729,
     "end_time": "2025-04-20T07:49:44.719063",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.695334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 10. RAG (Retrieval-Augmented Generation) ---\n",
    "def rag_query(query: str, chroma_client: Client, k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Performs a Retrieval-Augmented Generation (RAG) query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        chroma_client (Client): The ChromaDB client connected to the vector store.\n",
    "        k (int, optional): The number of top relevant documents to retrieve. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        str: An answer to the query generated based on the retrieved relevant documents.\n",
    "             Returns \"Vector store not initialized.\" if the client is None, or\n",
    "             \"No relevant documents found for the query.\" if no results are retrieved.\n",
    "    \"\"\"\n",
    "    if not chroma_client:\n",
    "        return \"Vector store not initialized.\"\n",
    "    results = vector_search_improved(chroma_client, query, k=k)\n",
    "    if not results:\n",
    "        return \"No relevant documents found for the query.\"\n",
    "    context = \"\\n\\n\".join([f\"Paper Excerpt:\\n{textwrap.shorten(res['document'], width=800, placeholder='...')}\" for res in results])\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following research paper excerpts, answer the query: '{query}'\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10fc6505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.752092Z",
     "iopub.status.busy": "2025-04-20T07:49:44.751275Z",
     "iopub.status.idle": "2025-04-20T07:49:44.763192Z",
     "shell.execute_reply": "2025-04-20T07:49:44.762112Z"
    },
    "papermill": {
     "duration": 0.029877,
     "end_time": "2025-04-20T07:49:44.764825",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.734948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 11. Vector Search (Improved) ---\n",
    "def initialize_vector_store(csv_path: str, pdf_folder: str) -> Client:\n",
    "    \"\"\"\n",
    "    Initializes a ChromaDB vector store with research papers from a CSV metadata file\n",
    "    and corresponding PDF files.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): The path to the CSV file containing paper metadata (filename, title, abstract).\n",
    "        pdf_folder (str): The path to the folder containing the PDF files.\n",
    "\n",
    "    Returns:\n",
    "        Client: The initialized ChromaDB client with the \"research_papers\" collection.\n",
    "                Returns the client even if no valid documents are added.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV metadata file is not found.\n",
    "        Exception: If an error occurs while reading the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        corpus_metadata = pd.read_csv(csv_path, quoting=csv.QUOTE_ALL, on_bad_lines=\"warn\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading corpus_metadata.csv: {e}\")\n",
    "        raise\n",
    "    documents_data = []\n",
    "    for index, row in corpus_metadata.iterrows():\n",
    "        filename_raw = row['filename']\n",
    "        title = row['title']\n",
    "        abstract = row['abstract']\n",
    "        if isinstance(filename_raw, str):\n",
    "            filename = filename_raw.strip()\n",
    "            filepath = os.path.join(pdf_folder, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                try:\n",
    "                    doc = create_document(filepath, title, abstract)\n",
    "                    documents_data.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing {filepath}: {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: PDF file does not exist: {filepath}\")\n",
    "        else:\n",
    "            print(f\"Warning: Skipping row {index} due to invalid filename: {filename_raw}\")\n",
    "    client = Client()\n",
    "    collection = client.get_or_create_collection(\"research_papers\")\n",
    "    texts = [d['content'] for d in documents_data]\n",
    "    metadatas = [d['metadata'] for d in documents_data]\n",
    "    if not texts:\n",
    "        print(\"Warning: No valid documents to add to vector store.\")\n",
    "        return client\n",
    "    embeddings_list = generate_embeddings(texts)\n",
    "    collection.add(\n",
    "        documents=texts,\n",
    "        metadatas=metadatas,\n",
    "        embeddings=embeddings_list,\n",
    "        ids=[f\"doc_{i}\" for i in range(len(documents_data))]\n",
    "    )\n",
    "    return client\n",
    "\n",
    "def vector_search_improved(chroma_client: Client, query: str, k: int = 4) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Performs a semantic vector search on the ChromaDB vector store.\n",
    "\n",
    "    Args:\n",
    "        chroma_client (Client): The ChromaDB client connected to the vector store.\n",
    "        query (str): The search query string.\n",
    "        k (int, optional): The number of top results to retrieve. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, where each dictionary contains information\n",
    "                     about a retrieved document, including its 'id', 'distance', 'document'\n",
    "                     content, and associated 'metadata'.\n",
    "    \"\"\"\n",
    "    collection = chroma_client.get_collection(\"research_papers\")\n",
    "    response = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=query,\n",
    "        task_type=\"retrieval_query\"\n",
    "    )\n",
    "    query_embedding = response[\"embedding\"]\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k\n",
    "    )\n",
    "    return [\n",
    "        {\n",
    "            \"id\": results['ids'][0][i],\n",
    "            \"distance\": results['distances'][0][i],\n",
    "            \"document\": results['documents'][0][i],\n",
    "            \"metadata\": results['metadatas'][0][i]\n",
    "        }\n",
    "        for i in range(len(results['ids'][0]))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3767ff9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.862802Z",
     "iopub.status.busy": "2025-04-20T07:49:44.862394Z",
     "iopub.status.idle": "2025-04-20T07:49:44.868087Z",
     "shell.execute_reply": "2025-04-20T07:49:44.867189Z"
    },
    "papermill": {
     "duration": 0.023093,
     "end_time": "2025-04-20T07:49:44.869660",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.846567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 12. MLOps Monitoring ---\n",
    "def monitor_performance(scores: dict) -> str:\n",
    "    \"\"\"\n",
    "    Monitors the performance of a summarization model based on ROUGE scores.\n",
    "\n",
    "    Args:\n",
    "        scores (dict): A dictionary of ROUGE scores, typically the output of\n",
    "                       the `evaluate_summary` function. Expected to contain\n",
    "                       'rougeL' with an 'fmeasure'.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the performance is within acceptable limits\n",
    "             or below the threshold. Returns an error message if the ROUGE scores\n",
    "             are not in the expected format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rouge_l_f1 = scores.get(\"rougeL\", {}).get(\"fmeasure\", 0)\n",
    "        if rouge_l_f1 < 0.45:\n",
    "            return \"Performance below acceptable threshold (ROUGE-L F1 < 0.45). Consider further investigation or retraining.\"\n",
    "        return \"Performance within acceptable limits (ROUGE-L F1 >= 0.45).\"\n",
    "    except KeyError:\n",
    "        print(\"Error: ROUGE scores not in expected format.\")\n",
    "        return \"Performance evaluation failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "871ab64b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.900809Z",
     "iopub.status.busy": "2025-04-20T07:49:44.900466Z",
     "iopub.status.idle": "2025-04-20T07:49:44.912313Z",
     "shell.execute_reply": "2025-04-20T07:49:44.911237Z"
    },
    "papermill": {
     "duration": 0.029627,
     "end_time": "2025-04-20T07:49:44.913843",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.884216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "def main():\n",
    "    csv_path = \"/kaggle/input/metadata/corpus_metadata.csv\"\n",
    "    pdf_folder = \"/kaggle/input/research-papers-corpus\"\n",
    "    sample_paper_folder = \"/kaggle/input/sample-paper\"\n",
    "    paper_file = \"Deja_Vu_Multilingual_LLM_Evaluation_through_the_Lens_of_Machine_Translation_Evaluation.pdf\"\n",
    "    paper_path = os.path.join(sample_paper_folder, paper_file)\n",
    "\n",
    "    # Verify file paths\n",
    "    if not os.path.exists(paper_path):\n",
    "        print(f\"Error: Sample paper not found at {paper_path}. Please ensure the path is correct.\")\n",
    "        return\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Error: corpus_metadata.csv not found at {csv_path}.\")\n",
    "        return\n",
    "\n",
    "    # Initialize vector store\n",
    "    vector_client = initialize_vector_store(csv_path, pdf_folder)\n",
    "\n",
    "    # Extract sample paper text\n",
    "    sample_paper_text = extract_sample_pdf_text(paper_path, max_chars=6000)\n",
    "\n",
    "    # 1. Structured Output\n",
    "    print(\"\\n--- 1. Structured Output ---\")\n",
    "    structured_output = generate_structured_output(sample_paper_text)\n",
    "    print(json.dumps(structured_output, indent=2))\n",
    "\n",
    "    # 2. Few-Shot Prompting\n",
    "    print(\"\\n--- 2. Few-Shot Prompting ---\")\n",
    "    corpus_metadata = pd.read_csv(csv_path, quoting=csv.QUOTE_ALL, on_bad_lines=\"warn\")\n",
    "    sample_abstracts = corpus_metadata[['title', 'abstract']].head(2).to_dict('records')\n",
    "    few_shot_summary_result = few_shot_summary(sample_abstracts, sample_paper_text)\n",
    "    print(f\"Summary: {few_shot_summary_result}\")\n",
    "\n",
    "    # 3. Document Understanding\n",
    "    print(\"\\n--- 3. Document Understanding ---\")\n",
    "    document_understanding_result = document_understanding(sample_paper_text)\n",
    "    print(document_understanding_result)\n",
    "\n",
    "    # 4. Agents\n",
    "    print(\"\\n--- 4. Agents (Simulated) ---\")\n",
    "    agent_query = \"Multilingual LLM evaluation methods\"\n",
    "    agent_retrieval_result = agent_retrieve_paper(agent_query, vector_client)\n",
    "    print(agent_retrieval_result)\n",
    "\n",
    "    # 5. Long Context Window\n",
    "    print(\"\\n--- 5. Long Context Window ---\")\n",
    "    long_context_result = long_context_summary(extract_text_from_pdf(paper_path))\n",
    "    print(long_context_result)\n",
    "\n",
    "    # 6. Context Caching\n",
    "    print(\"\\n--- 6. Context Caching ---\")\n",
    "    context_caching_result = context_caching_summary(sample_paper_text)\n",
    "    print(context_caching_result)\n",
    "\n",
    "    # 7. Gen AI Evaluation\n",
    "    print(\"\\n--- 7. Gen AI Evaluation ---\")\n",
    "    ground_truth_summary = \"This paper advocates adopting machine translation evaluation practices to enhance multilingual LLM evaluation with a meta-evaluation checklist.\"\n",
    "    model_summary_for_eval = few_shot_summary(sample_abstracts, sample_paper_text)\n",
    "    evaluation_scores = evaluate_summary(ground_truth_summary, model_summary_for_eval)\n",
    "    print(f\"Evaluation Scores:\\n{json.dumps(evaluation_scores, indent=2)}\")\n",
    "\n",
    "    # 8. Grounding\n",
    "    print(\"\\n--- 8. Grounding ---\")\n",
    "    grounding_result = grounding_analysis(sample_paper_text)\n",
    "    print(grounding_result)\n",
    "\n",
    "    # 9. Embeddings\n",
    "    print(\"\\n--- 9. Embeddings ---\")\n",
    "    sample_embedding_text = sample_paper_text[:8192]\n",
    "    embeddings_result = generate_embeddings([sample_embedding_text])\n",
    "    print(f\"Embeddings for sample paper (first 10 values):\\n{embeddings_result[0][:10]}\")\n",
    "\n",
    "    # 10. RAG\n",
    "    print(\"\\n--- 10. RAG (Retrieval-Augmented Generation) ---\")\n",
    "    rag_query_str = \"Evaluation metrics for multilingual LLMs\"\n",
    "    rag_result = rag_query(rag_query_str, vector_client)\n",
    "    print(rag_result)\n",
    "\n",
    "    # 11. Vector Search\n",
    "    print(\"\\n--- 11. Vector Search (Improved) ---\")\n",
    "    search_query = \"multilingual evaluation\"\n",
    "    search_results = vector_search_improved(vector_client, search_query)\n",
    "    for i, result in enumerate(search_results):\n",
    "        print(f\"Result {i+1}:\")\n",
    "        print(f\"ID: {result['id']}\")\n",
    "        print(f\"Distance: {result['distance']}\")\n",
    "        print(f\"Content Preview: {textwrap.shorten(result['document'], width=300, placeholder='...')}\")\n",
    "        print(f\"Metadata: {json.dumps(result['metadata'], indent=2)}\")\n",
    "        print(\"-\" * 20)\n",
    "    vector_client.delete_collection(\"research_papers\")  # Clean up\n",
    "\n",
    "    # 12. MLOps Monitoring\n",
    "    print(\"\\n--- 12. MLOps Monitoring ---\")\n",
    "    monitoring_result = monitor_performance(evaluation_scores)\n",
    "    print(f\"Monitoring Result: {monitoring_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3df97b65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T07:49:44.945332Z",
     "iopub.status.busy": "2025-04-20T07:49:44.944391Z",
     "iopub.status.idle": "2025-04-20T07:50:26.171890Z",
     "shell.execute_reply": "2025-04-20T07:50:26.170225Z"
    },
    "papermill": {
     "duration": 41.245018,
     "end_time": "2025-04-20T07:50:26.173608",
     "exception": false,
     "start_time": "2025-04-20T07:49:44.928590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Structured Output ---\n",
      "{\n",
      "  \"title\": \"D\\u00e9j\\u00e0 Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation\",\n",
      "  \"authors\": [\n",
      "    \"Julia Kreutzer\",\n",
      "    \"Eleftheria Briakou\",\n",
      "    \"Sweta Agrawal\",\n",
      "    \"Marzieh Fadaee\",\n",
      "    \"Kocmi Tom\"\n",
      "  ],\n",
      "  \"key_findings\": [\n",
      "    \"Current generative evaluation approaches for multilingual models lack nuances in reporting, reproducibility, standardization, robustness and reliability, and most notably, meta-evaluation.\",\n",
      "    \"Multilingual models shine especially in generative tasks, outperforming monolingual models across the bench.\",\n",
      "    \"Many existing benchmarks have reached saturation and are not sufficiently separating models, making them unreliable predictors of generative abilities of mLLMs.\",\n",
      "    \"Identified five concrete evaluation principles lacking in mLLM evaluations but established in MT.\",\n",
      "    \"Established prerequisites necessary for meta-evaluations of mLLMs.\"\n",
      "  ],\n",
      "  \"methodology\": \"The paper draws parallels between multilingual large language model (mLLM) evaluation and machine translation (MT) evaluation, using targeted experiments to demonstrate how best practices from MT evaluation can improve the understanding of quality differences between models. The research assesses current benchmarks and their adoption in model releases to identify challenges in generative mLLM evaluation. It also compiles a non-exhaustive list of open multilingual generative benchmarks to survey the mLLM landscape.\",\n",
      "  \"implications\": \"The research provides actionable recommendations for mLLM research and development, aiming to steer the field towards more reliable, expressive, and rigorous evaluations. By bridging the gap between mLLM and MT evaluation, the study helps in improving the comprehensiveness, scientific rigor, and consistent adoption of evaluation practices, which can guide mLLM development more effectively.\"\n",
      "}\n",
      "\n",
      "--- 2. Few-Shot Prompting ---\n",
      "Summary: This paper proposes leveraging established machine translation evaluation techniques to improve the comprehensiveness, rigor, and consistency of multilingual LLM evaluation.\n",
      "\n",
      "--- 3. Document Understanding ---\n",
      "Okay, here's a structured breakdown of the provided research paper text, including the requested sections:\n",
      "\n",
      "**Title:** Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation\n",
      "\n",
      "**Authors:** Julia Kreutzer, Eleftheria Briakou, Sweta Agrawal, Marzieh Fadaee, Kocmi Tom\n",
      "\n",
      "**Affiliations:** Cohere Labs, Google, Cohere\n",
      "\n",
      "**Abstract:** The paper addresses the lack of comprehensive, rigorous, and consistent evaluation practices for multilingual large language models (mLLMs). It draws parallels with machine translation (MT) evaluation, a more mature field, and suggests adopting MT evaluation best practices to improve mLLM evaluation. The paper identifies essential components for robust meta-evaluation of mLLMs and provides actionable recommendations for mLLM research and development.\n",
      "\n",
      "**1. Introduction:**\n",
      "\n",
      "*   **Problem:** Evaluating LLMs in multilingual contexts faces challenges like benchmark contamination, label noise, and reliability issues, which are amplified across multiple languages. Existing benchmarks are often saturated and unreliable for assessing generative abilities, which are key in real-world applications. Current generative evaluation approaches for multilingual models lack nuances in reporting, reproducibility, standardization, robustness, reliability, and meta-evaluation.\n",
      "*   **Solution:** The paper connects mLLM evaluation with machine translation (MT) evaluation, a field with a longer history and established practices in multilingual generation evaluation.\n",
      "*   **Approach:** The paper identifies challenges in current mLLM evaluation, highlights key evaluation principles from MT, and establishes prerequisites for meta-evaluations. The findings are distilled into an actionable checklist.\n",
      "\n",
      "**2. The Status Quo of Multilingual LLM Generation Evaluation:**\n",
      "\n",
      "*   **Goal:** The section reviews current multilingual generative benchmarks.\n",
      "*   **Method:** A non-exhaustive list of benchmarks is compiled in Table 1 to assess the mLLM field.\n",
      "*   **Next Step:** Trends are summarized and linked to proposed strategies from MT evaluation research in Sections 3 and meta-evaluation in Section 4.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "*   Current evaluation practices for multilingual LLMs (mLLMs) are inadequate, lacking comprehensiveness, rigor, and consistency.\n",
      "*   Machine translation (MT) evaluation offers valuable insights and best practices that can be adapted to improve mLLM evaluation.\n",
      "*   Existing multilingual benchmarks often rely on translated English data and are limited in their ability to assess generative abilities.\n",
      "*   Meta-evaluation, the evaluation of evaluation methods themselves, is essential for ensuring the robustness and reliability of mLLM evaluations.\n",
      "*   The paper provides a checklist of actionable recommendations to guide mLLM research towards more reliable and rigorous evaluations.\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "The research primarily uses an analytical approach. It involves reviewing existing mLLM evaluation benchmarks, identifying gaps in current practices, and drawing parallels with established methods in machine translation (MT) evaluation. The paper proposes a set of principles and prerequisites based on MT evaluation best practices and synthesizes these into actionable recommendations for improving mLLM evaluation. Targeted experiments are mentioned within the abstract that validate quality differences between models.\n",
      "\n",
      "**In Simpler Terms:**\n",
      "\n",
      "Imagine you're trying to teach a computer to speak multiple languages. We're pretty good at testing how well computers translate between languages (that's machine translation). But we're not as good at testing how well new multilingual AI models can *generate* text in different languages. This paper argues that we can learn a lot from how we test machine translation to improve how we test these new AI models, especially when it comes to making sure our tests are fair, reliable, and really measure what we think they measure. The paper then provides a guide to help researchers better evaluate multilingual AI.\n",
      "\n",
      "--- 4. Agents (Simulated) ---\n",
      "\n",
      "        Retrieved Paper:\n",
      "        Content Preview: Unsupervised Cross-lingual Representation Learning at Scale Alexis Conneau\u0003Kartikay Khandelwal\u0003 Naman Goyal Vishrav Chaudhary Guillaume Wenzek Francisco Guzm ´an Edouard Grave Myle Ott Luke Zettlemoyer Veselin Stoyanov Facebook AI Abstract This paper shows that pretraining multilingual language models at scale leads to signiﬁcant performance gains for a wide range of cross- lingual transfer tasks. We train a Transformer- based masked language model on one hundred languages, using more than two terabytes of ﬁl- tered CommonCrawl data. Our model, dubbed XLM-R , signiﬁcantly outperforms...\n",
      "        Metadata: {\n",
      "  \"abstract\": \"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of crosslingual transfer tasks. We train a Transformerbased masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available\",\n",
      "  \"filename\": \"Unsupervised_Cross-lingual_Representation_Learning_at_Scale.pdf\",\n",
      "  \"title\": \"Unsupervised Cross-lingual Representation Learning at Scale\"\n",
      "}\n",
      "        \n",
      "\n",
      "--- 5. Long Context Window ---\n",
      "This research paper, titled \"Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation,\" argues that the current evaluation practices for multilingual Large Language Models (mLLMs) lack comprehensiveness, rigor, and standardization, hindering their development. To address this, the authors draw parallels with the field of machine translation (MT) evaluation, which has a longer history and more established methodologies.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1.  **Current mLLM evaluation practices are lacking:** The paper identifies several shortcomings in current mLLM evaluation, including:\n",
      "    *   **Over-reliance on translated benchmarks:** Many benchmarks are simply translations of English datasets, which can introduce biases and not accurately reflect the nuances of other languages and cultures.\n",
      "    *   **Small test sets:** The size of many test sets is small, raising concerns about statistical power and the ability to reliably differentiate between models.\n",
      "    *   **Lack of development sets:** The absence of development sets for tuning can lead to overfitting and reduced generalizability.\n",
      "    *   **Insufficient qualitative analysis:** Evaluation reports often lack detailed qualitative insights into model performance, focusing primarily on aggregate metrics.\n",
      "    *   **Inconsistent reporting:** There is a lack of standardization in evaluation reporting, including the choice of evaluation metrics, prompting strategies, and language aggregation methods, making it difficult to compare results across different studies.\n",
      "\n",
      "2.  **Machine Translation Evaluation offers valuable insights:**  The paper highlights the progress in MT evaluation, especially in areas like:\n",
      "    *   **Diverse Evaluation Metrics:** MT evaluation has a wide range of both automatic and human-based evaluation metrics.\n",
      "    *   **Meta-evaluation:** MT has a established tradition of evaluating the evaluations themselves, establishing standards of quality, reliability and transparency.\n",
      "    *   **Development of standards and best practices:** The field of MT evaluation has developed transparent reporting standards and reliable evaluations for multilingual generative models.\n",
      "\n",
      "3.  **Need for Meta-Evaluation in mLLM:**  For mLLM to mature as a field, meta-evaluation must be incorporated to ensure the reliability of the evaluation methods themselves.\n",
      "\n",
      "**Implications and Recommendations:**\n",
      "\n",
      "The paper proposes that the mLLM evaluation field can learn from the experiences of MT evaluation. Specifically, the authors suggest that mLLM researchers should:\n",
      "\n",
      "*   **Consider cultural representation:** When developing multilingual benchmarks, prioritize datasets curated directly in the target languages to ensure cultural representativeness and reduce biases.\n",
      "*   **Increase test set size:** Aim for larger test sets to improve statistical power and the ability to reliably differentiate between models.\n",
      "*   **Incorporate development sets:** Include development sets for tuning to mitigate overfitting and improve generalizability.\n",
      "*   **Emphasize qualitative analysis:** Supplement quantitative metrics with detailed qualitative analysis of model performance to gain deeper insights.\n",
      "*   **Standardize reporting:** Adopt standardized evaluation reporting practices, including clear documentation of evaluation metrics, prompting strategies, and language aggregation methods, to facilitate cross-paper comparisons.\n",
      "*   **Incorporate Meta-Evaluation** Conduct meta-evaluations of evaluation methods to ensure that the evaluation methods themselves are reliable.\n",
      "*   **Adopt a checklist for mLLM research:**  Appendix I (available on Github) to help steer mLLM development towards more reliable, expressive, and rigorous evaluations.\n",
      "\n",
      "In essence, the paper advocates for a more rigorous and standardized approach to mLLM evaluation, drawing on the lessons learned from the MT evaluation field. This, in turn, will facilitate more meaningful progress in mLLM development and deployment.\n",
      "\n",
      "--- 6. Context Caching ---\n",
      "**Core Argument:**\n",
      "\n",
      "The paper argues that the current evaluation practices for multilingual large language models (mLLMs) lag behind the advancements in the models themselves. They lack the comprehensiveness, rigor, and consistency found in machine translation (MT) evaluation. By drawing parallels between the two fields, the paper proposes leveraging established MT evaluation techniques to improve the evaluation of generative capabilities in mLLMs, leading to more reliable and informative model development.\n",
      "\n",
      "**Key Contributions:**\n",
      "\n",
      "*   **Identifies Deficiencies in mLLM Evaluation:** Highlights shortcomings in existing mLLM evaluation, including a lack of nuance in reporting, reproducibility, standardization, robustness, reliability, and meta-evaluation.\n",
      "*   **Draws Parallels with MT Evaluation:** Establishes a connection between mLLM and MT evaluation, demonstrating how solutions developed in MT can address challenges in mLLM evaluation.\n",
      "*   **Presents Concrete Evaluation Principles:** Identifies five key evaluation principles from MT that are lacking in mLLM evaluations.\n",
      "*   **Emphasizes the Importance of Meta-Evaluation:** Stresses the need for rigorous meta-evaluation (evaluation of evaluations) to ensure the reliability and validity of mLLM evaluation methods.\n",
      "*   **Provides an Actionable Checklist:** Offers a practical checklist for mLLM research to guide the development of more reliable, expressive, and rigorous evaluations.\n",
      "\n",
      "--- 7. Gen AI Evaluation ---\n",
      "Evaluation Scores:\n",
      "{\n",
      "  \"rouge1\": {\n",
      "    \"precision\": 0.8461538461538461,\n",
      "    \"recall\": 0.6111111111111112,\n",
      "    \"fmeasure\": 0.7096774193548387\n",
      "  },\n",
      "  \"rougeL\": {\n",
      "    \"precision\": 0.8461538461538461,\n",
      "    \"recall\": 0.6111111111111112,\n",
      "    \"fmeasure\": 0.7096774193548387\n",
      "  }\n",
      "}\n",
      "\n",
      "--- 8. Grounding ---\n",
      "Okay, let's break down the mentions of existing research, benchmarks, datasets, and how this paper positions itself in relation to them.\n",
      "\n",
      "**1.  General LLM Evaluation Challenges (Monolingual and Multilingual):**\n",
      "\n",
      "*   **Mentions:** The paper opens by acknowledging challenges inherited from monolingual LLM evaluation, including:\n",
      "    *   Benchmark contamination (Yang et al., 2023; Deng et al., 2024; Dong et al., 2024; Li et al., 2024; Ni et al., 2025)\n",
      "    *   Label noise (Vendrow et al., 2025)\n",
      "    *   Costs vs. coverage trade-offs (Zhang et al., 2024a)\n",
      "    *   Standardization, reliability, and diversity issues (McIntosh et al., 2024)\n",
      "*   **How the paper builds upon/differs:**  It acknowledges these problems but argues that they are amplified in the multilingual context. The paper doesn't propose direct solutions to *these* specific monolingual issues; rather, it focuses on the *additional* challenges that arise *on top* of these when dealing with multiple languages and generative tasks.\n",
      "\n",
      "**2.  Existing Multilingual Benchmarks (Pre-Decoder-Only LLM Era):**\n",
      "\n",
      "*   **Mentions:**  The paper notes that older classification benchmarks from cross/multilingual studies can be reused:\n",
      "    *   Hu et al., 2020\n",
      "    *   Ruder et al., 2021\n",
      "    *   Liang et al., 2020\n",
      "    *   Ahuja et al., 2023\n",
      "    *   Asai et al., 2024\n",
      "*   **How the paper builds upon/differs:** The paper *critiques* these benchmarks. It argues that many have reached \"saturation\" (Kiela et al., 2021; 2023), don't sufficiently differentiate between models (Zhang et al., 2024c), and are unreliable predictors of *generative* abilities (Üstün et al., 2024). The paper implies that these benchmarks are more suitable for *knowledge testing* than evaluating the core generative capabilities now expected of modern LLMs.\n",
      "\n",
      "**3.  Generative Evaluation for LLMs (Generally):**\n",
      "\n",
      "*   **Mentions:** The paper points out that generative abilities are key in real-world applications (Tamkin et al., 2024), and have moved into the spotlight of LLM evaluations.\n",
      "    *   Dubois et al., 2023\n",
      "    *   Chiang et al., 2024\n",
      "    *   Lin et al., 2024\n",
      "*   **How the paper builds upon/differs:** The paper argues that evaluations for *multilingual generative* tasks are still lacking in nuances in reporting, reproducibility, standardization, robustness and reliability, and most notably, meta-evaluation.\n",
      "\n",
      "**4.  Machine Translation (MT) Evaluation (as a Model):**\n",
      "\n",
      "*   **Mentions:** The paper heavily references machine translation evaluation as a mature field with established practices.  It cites numerous works related to:\n",
      "    *   Automatic metrics (Papineni et al., 2002; Koehn & Monz, 2006a; Lavie & Agarwal, 2007; Stanojević & Sima’an, 2014; Popović, 2015; Rei et al., 2020)\n",
      "    *   Human judgments (Vilar et al., 2007; Birch & Osborne, 2010; Lopez, 2012; Graham et al., 2013; Freitag et al., 2021; Kocmi et al., 2024b)\n",
      "    *   Meta-evaluation (Callison-Burch et al., 2007; 2008; Macháček & Bojar, 2013; Post, 2018; Mathur et al., 2020; Amrhein et al., 2022; Deutsch et al., 2023)\n",
      "    *   WMT\n",
      "*   **How the paper builds upon/differs:** This is the core of the paper's contribution.  It *explicitly draws parallels* between the challenges faced in early MT evaluation and the current state of multilingual LLM evaluation.  It argues that the MT field has developed solutions, standards, and a framework for meta-evaluation that can be adapted and applied to the LLM space. The paper *advocates for transferring the lessons learned from MT evaluation to multilingual LLM evaluation*.\n",
      "\n",
      "**5. XLSum:**\n",
      "\n",
      "*   **Mentions:** \"Only XLSum(Hasan et al.\"\n",
      "*   **How the paper builds upon/differs:** The paper seems to be building a list of open multilingual generative benchmarks (Table 1) in section 2. It uses XLSum as an example of a task that relies on the translation of the original English benchmark for multilingual expansion.\n",
      "\n",
      "**In summary:**\n",
      "\n",
      "This paper doesn't present itself as a fundamentally new evaluation *method*, but rather as a bridge between the well-established field of MT evaluation and the rapidly evolving area of multilingual LLM evaluation. It identifies shortcomings in current multilingual LLM evaluation practices, argues that MT evaluation offers valuable lessons, and aims to provide actionable recommendations based on those lessons. It builds upon an understanding of existing LLM benchmarks and their limitations, as well as draws a direct comparison to standards, metrics, and approaches used in the machine translation domain.\n",
      "\n",
      "--- 9. Embeddings ---\n",
      "Embeddings for sample paper (first 10 values):\n",
      "[0.0027538654, -0.057674177, -0.06612257, 0.04715755, 0.06773807, 0.020436697, 0.017430097, 0.0009726794, 0.022560172, 0.007080286]\n",
      "\n",
      "--- 10. RAG (Retrieval-Augmented Generation) ---\n",
      "Based on the provided excerpts, evaluation metrics for multilingual LLMs include:\n",
      "\n",
      "*   **Accuracy:** Used in the XNLI task, as seen with XLM-R achieving +14.6% average accuracy compared to mBERT.\n",
      "*   **F1 Score:** Used in MLQA and NER tasks. XLM-R outperformed mBERT with +13% average F1 score on MLQA and +2.4% F1 score on NER.\n",
      "*   **BLEU:** Used for automatic evaluation of machine translation.\n",
      "*   **XTREME:** A multilingual benchmark used to evaluate cross-lingual transfer learning.\n",
      "\n",
      "--- 11. Vector Search (Improved) ---\n",
      "Result 1:\n",
      "ID: doc_8\n",
      "Distance: 0.5932642817497253\n",
      "Content Preview: XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation Sebastian Ruder1, Noah Constant2, Jan Botha2, Aditya Siddhant2, Orhan Firat2, Jinlan Fu3, Pengfei Liu4, Junjie Hu4, Dan Garrette2, Graham Neubig4, Melvin Johnson2 1DeepMind2Google Research3Fudan University4Carnegie Mellon...\n",
      "Metadata: {\n",
      "  \"title\": \"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation\",\n",
      "  \"abstract\": \"Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to humanlevel performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite (MULTICHECKLIST) and finegrained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models\",\n",
      "  \"filename\": \"XTREME-R_Towards_More_Challenging_and_Nuanced_Multilingual_Evaluation.pdf\"\n",
      "}\n",
      "--------------------\n",
      "Result 2:\n",
      "ID: doc_7\n",
      "Distance: 0.6543558239936829\n",
      "Content Preview: Unsupervised Cross-lingual Representation Learning at Scale Alexis Conneau\u0003Kartikay Khandelwal\u0003 Naman Goyal Vishrav Chaudhary Guillaume Wenzek Francisco Guzm ´an Edouard Grave Myle Ott Luke Zettlemoyer Veselin Stoyanov Facebook AI Abstract This paper shows that pretraining multilingual language...\n",
      "Metadata: {\n",
      "  \"abstract\": \"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of crosslingual transfer tasks. We train a Transformerbased masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available\",\n",
      "  \"filename\": \"Unsupervised_Cross-lingual_Representation_Learning_at_Scale.pdf\",\n",
      "  \"title\": \"Unsupervised Cross-lingual Representation Learning at Scale\"\n",
      "}\n",
      "--------------------\n",
      "Result 3:\n",
      "ID: doc_9\n",
      "Distance: 0.6785098910331726\n",
      "Content Preview: Proceedings of the Tenth Workshop on Statistical Machine Translation , pages 392–395, Lisboa, Portugal, 17-18 September 2015. c 2015 Association for Computational Linguistics. CHRF: character n-gram F-score for automatic MT evaluation Maja Popovi ´c Humboldt University of Berlin Germany...\n",
      "Metadata: {\n",
      "  \"title\": \"CHRF: character n-gram F-score for automatic MT evaluation\",\n",
      "  \"filename\": \"chrF_Character_n-gram_F-score_for_Automatic_MT_Evaluation.pdf\",\n",
      "  \"abstract\": \"We propose the use of character n-gram F-score for automatic evaluation of machine translation output. Character ngrams have already been used as a part of more complex metrics, but their individual potential has not been investigated yet. We report system-level correlations with human rankings for 6-gram F1-score (CHRF) on the WMT12, WMT13 and WMT14 data as well as segment-level correlation for 6- gram F1 (CHRF) and F3-scores (CHRF3) on WMT14 data for all available target languages. The results are very promising, especially for the CHRF3 score \\u2013 for translation from English, this variant showed the highest segment-level correlations outperforming even the best metrics on the WMT14 shared evaluation task\"\n",
      "}\n",
      "--------------------\n",
      "Result 4:\n",
      "ID: doc_2\n",
      "Distance: 0.7105891108512878\n",
      "Content Preview: BLEU:aMethodforAutomaticEvaluationofMachineTranslation KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu IBMT.J.WatsonResearch Center YorktownHeights,NY10598,USA fpapineni,rouk os,toddward,weijing g@us.ibm.com Abstract Human evaluations ofmachine translation areextensi vebutexpensi ve.Human...\n",
      "Metadata: {\n",
      "  \"filename\": \"BLEU_A_Method_for_Automatic_Evaluation_of_Machine_Translation.pdf\",\n",
      "  \"title\": \"BLEU: a Method for Automatic Evaluation of Machine Translation\",\n",
      "  \"abstract\": \"Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations\"\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- 12. MLOps Monitoring ---\n",
      "Monitoring Result: Performance within acceptable limits (ROUGE-L F1 >= 0.45).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f203f00",
   "metadata": {
    "papermill": {
     "duration": 0.01619,
     "end_time": "2025-04-20T07:50:26.206476",
     "exception": false,
     "start_time": "2025-04-20T07:50:26.190286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7180687,
     "sourceId": 11459880,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7180701,
     "sourceId": 11459900,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7188312,
     "sourceId": 11470405,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 92.264762,
   "end_time": "2025-04-20T07:50:28.844592",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T07:48:56.579830",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
